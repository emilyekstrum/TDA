{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc4320be",
   "metadata": {},
   "source": [
    "# Persistence Barcodes from Neural Spike Data\n",
    "\n",
    "Generates persistence barcodes using fuzzy UMAP algorithm from the toroidal grid cell paper.\n",
    "\n",
    "- **Input:** Cleaned spike data automatically loaded from the TDA data manager\n",
    "- **Output:** Persistence barcodes for homology groups H0-H2\n",
    "- **Method:** Fuzzy simplicial sets → distance matrix → Ripser TDA\n",
    "\n",
    "Author: @emilyekstrum & Gardner et al. (2022)\n",
    "<br> 11/25/25\n",
    "\n",
    "**Reference:** Gardner, R.J., Hermansen, E., Pachitariu, M. et al. Toroidal topology of population activity in grid cells. *Nature* 602, 123–128 (2022). https://doi.org/10.1038/s41586-021-04268-7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3febdfb",
   "metadata": {},
   "source": [
    "## Fuzzy UMAP Methodology Overview\n",
    "\n",
    "The pipeline implements the fuzzy UMAP approach from Gardner et al.:\n",
    "\n",
    "1. **Fuzzy KNN Graph Construction**: Build weighted graph using greedy farthest point sampling\n",
    "   - Select most connected points, down-weight nearby points\n",
    "   - Ensures coverage of less-dense data regions\n",
    "\n",
    "2. **Distance & KNN Graph**: Compute pairwise distances and k-nearest neighbors\n",
    "\n",
    "3. **UMAP-style Scale Estimation**: Find optimal sigma (scaling) and rho (nearest neighbor distance)\n",
    "   - Ensures soft membership ≥ log₂(k) for topology preservation\n",
    "\n",
    "4. **Fuzzy Simplicial Set**: Convert KNN → sparse adjacency matrix → directed fuzzy graph\n",
    "\n",
    "5. **Greedy Sampling**: Identify highly connected points using fuzzy similarities\n",
    "\n",
    "6. **TDA**: Use fuzzy similarity matrix as input to Ripser for persistence computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4db90ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import glob\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from ripser import Rips, ripser\n",
    "from scipy import stats, signal, optimize\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.linalg import eigh\n",
    "from scipy.sparse.linalg import lsmr\n",
    "from sklearn import preprocessing\n",
    "\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "plt.style.use(['default', 'seaborn-v0_8-paper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa03b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spike data already available.\n",
      "Extracting persistence diagrams from all_dgms.zip\n",
      "Extracted to: /Users/emilyekstrum/repos/TDA/data/all_dgms\n",
      "TDA utilities loaded\n",
      "TDA Data Manager Summary\n",
      "Workspace root: /Users/emilyekstrum/repos/TDA\n",
      "Data directory: /Users/emilyekstrum/repos/TDA/data\n",
      "  - clean_spike_data_zip: clean_spike_data.zip\n",
      "  - clean_spike_data_dir: clean_spike_data\n",
      "      Contains 8 .pkl files\n",
      "  - cebra_examples: CEBRA_embedding_examples\n",
      "      Contains 5 .pkl files\n",
      "  - persistence_examples: persistence_diagram_examples\n",
      "      Contains 22 .pkl files\n",
      "  - all_dgms_zip: all_dgms.zip\n",
      "  X all_dgms_dir: all_dgms\n",
      "\n",
      "Available spike datasets (8):\n",
      "  • LGN_chromatic_gratings.pkl\n",
      "  • LGN_color_exchange.pkl\n",
      "  • LGN_drifting_gratings.pkl\n",
      "  • LGN_luminance_flash.pkl\n",
      "  • V1_chromatic_gratings.pkl\n",
      "  and 3 more\n"
     ]
    }
   ],
   "source": [
    "# Import TDA utilities\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the repo root to path if needed\n",
    "repo_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "try:\n",
    "    from tda_utils import TDADataManager, tda_manager\n",
    "    print(\"TDA utilities loaded\")\n",
    "except ImportError as e:\n",
    "    print(f\"Could not import TDA utilities: {e}\")\n",
    "    print(\"Make sure you're running from the TDA repository.\")\n",
    "    raise\n",
    "\n",
    "# Initialize or use the global data manager\n",
    "if tda_manager is not None:\n",
    "    data_manager = tda_manager\n",
    "else:\n",
    "    data_manager = TDADataManager()\n",
    "\n",
    "# Print summary of available data\n",
    "data_manager.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b54e16d",
   "metadata": {},
   "source": [
    "## 1. Load in spike data and inspect files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efb126d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple datasets available:\n",
      "  1. LGN_chromatic_gratings.pkl\n",
      "  2. LGN_color_exchange.pkl\n",
      "  3. LGN_drifting_gratings.pkl\n",
      "  4. LGN_luminance_flash.pkl\n",
      "  5. V1_chromatic_gratings.pkl\n",
      "  6. V1_color_exchange.pkl\n",
      "  7. V1_drifting_gratings.pkl\n",
      "  8. V1_luminance_flash.pkl\n",
      "\n",
      "Using the first one. To specify, use dataset_name parameter.\n",
      "Loading dataset: LGN_chromatic_gratings.pkl\n",
      "Loaded 3 recording sessions\n",
      "  Sessions: ['C155', 'C159', 'C161']\n",
      "  Data shape: torch.Size([136000, 15])\n",
      "Dataset: LGN_chromatic_gratings.pkl\n",
      "Sessions: 3\n",
      "Session IDs: ['C155', 'C159', 'C161']\n",
      "Data shapes:\n",
      "  C155: torch.Size([136000, 15]) (torch.Tensor)\n",
      "  C159: torch.Size([136000, 177]) (torch.Tensor)\n",
      "  C161: torch.Size([136000, 95]) (torch.Tensor)\n"
     ]
    }
   ],
   "source": [
    "# Load spike data \n",
    "try:\n",
    "    datas, recordings, dataset_info = data_manager.load_spike_data()\n",
    "    print(f\"Dataset: {dataset_info['filename']}\")\n",
    "    print(f\"Sessions: {dataset_info['n_sessions']}\")\n",
    "    print(f\"Session IDs: {recordings}\")\n",
    "    \n",
    "    print(\"Data shapes:\")\n",
    "    for name, X in zip(recordings, datas):\n",
    "        print(f\"  {name}: {X.shape} ({'torch.Tensor' if isinstance(X, torch.Tensor) else 'numpy.ndarray'})\")\n",
    "    \n",
    "    # To load a specific dataset, use:\n",
    "    # datas, recordings, dataset_info = data_manager.load_spike_data('LGN_color_exchange.pkl')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not load data: {e}\")\n",
    "    print(\"\\nTo fix issue:\")\n",
    "    print(\"1. Make sure you're in the TDA repository\")\n",
    "    print(\"2. Check that data/clean_spike_data.zip exists\")\n",
    "    print(\"3. Or manually add .pkl files to data/clean_spike_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ec16724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuzzy UMAP functions from Gardner et al. (2022)\n",
    "# RUN ME\n",
    "\n",
    "import numba\n",
    "from datetime import datetime \n",
    "import functools\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.ndimage import gaussian_filter, gaussian_filter1d, binary_dilation, binary_closing\n",
    "\n",
    "# umap functions\n",
    "@numba.njit(parallel=True, fastmath=True) \n",
    "def compute_membership_strengths(knn_indices, knn_dists, sigmas, rhos):\n",
    "    \"\"\"Compute fuzzy membership strengths for KNN graph.\"\"\"\n",
    "    n_samples = knn_indices.shape[0]\n",
    "    n_neighbors = knn_indices.shape[1]\n",
    "    rows = np.zeros((n_samples * n_neighbors), dtype=np.int64)\n",
    "    cols = np.zeros((n_samples * n_neighbors), dtype=np.int64)\n",
    "    vals = np.zeros((n_samples * n_neighbors), dtype=np.float64)\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_neighbors):\n",
    "            if knn_indices[i, j] == -1:\n",
    "                continue  # We didn't get the full knn for i\n",
    "            if knn_indices[i, j] == i:\n",
    "                val = 0.0\n",
    "            elif knn_dists[i, j] - rhos[i] <= 0.0:\n",
    "                val = 1.0\n",
    "            else:\n",
    "                val = np.exp(-((knn_dists[i, j] - rhos[i]) / (sigmas[i])))\n",
    "\n",
    "            rows[i * n_neighbors + j] = i\n",
    "            cols[i * n_neighbors + j] = knn_indices[i, j]\n",
    "            vals[i * n_neighbors + j] = val\n",
    "\n",
    "    return rows, cols, vals\n",
    "\n",
    "@numba.njit(fastmath=True)\n",
    "def smooth_knn_dist(distances, k, n_iter=64, local_connectivity=0.0, bandwidth=1.0):\n",
    "    \"\"\"Smooth KNN distances using UMAP methodology.\"\"\"\n",
    "    target = np.log2(k) * bandwidth\n",
    "    \n",
    "    rho = np.zeros(distances.shape[0])\n",
    "    result = np.zeros(distances.shape[0])\n",
    "\n",
    "    mean_distances = np.mean(distances)\n",
    "\n",
    "    for i in range(distances.shape[0]):\n",
    "        lo = 0.0\n",
    "        hi = np.inf\n",
    "        mid = 1.0\n",
    "\n",
    "        ith_distances = distances[i]\n",
    "        non_zero_dists = ith_distances[ith_distances > 0.0]\n",
    "        if non_zero_dists.shape[0] >= local_connectivity:\n",
    "            index = int(np.floor(local_connectivity))\n",
    "            interpolation = local_connectivity - index\n",
    "            if index > 0:\n",
    "                rho[i] = non_zero_dists[index - 1]\n",
    "                if interpolation > 1e-5:\n",
    "                    rho[i] += interpolation * (\n",
    "                        non_zero_dists[index] - non_zero_dists[index - 1]\n",
    "                    )\n",
    "            else:\n",
    "                rho[i] = interpolation * non_zero_dists[0]\n",
    "        elif non_zero_dists.shape[0] > 0:\n",
    "            rho[i] = np.max(non_zero_dists)\n",
    "\n",
    "        for n in range(n_iter):\n",
    "            psum = 0.0\n",
    "            for j in range(1, distances.shape[1]):\n",
    "                d = distances[i, j] - rho[i]\n",
    "                if d > 0:\n",
    "                    psum += np.exp(-(d / mid))\n",
    "                else:\n",
    "                    psum += 1.0\n",
    "\n",
    "            if np.fabs(psum - target) < 1e-5:\n",
    "                break\n",
    "\n",
    "            if psum > target:\n",
    "                hi = mid\n",
    "                mid = (lo + hi) / 2.0\n",
    "            else:\n",
    "                lo = mid\n",
    "                if hi == np.inf:\n",
    "                    mid *= 2\n",
    "                else:\n",
    "                    mid = (lo + hi) / 2.0\n",
    "        result[i] = mid\n",
    "\n",
    "        if rho[i] > 0.0:\n",
    "            mean_ith_distances = np.mean(ith_distances)\n",
    "            if result[i] < 1e-3 * mean_ith_distances:\n",
    "                result[i] = 1e-3 * mean_ith_distances\n",
    "        else:\n",
    "            if result[i] < 1e-3 * mean_distances:\n",
    "                result[i] = 1e-3 * mean_distances\n",
    "\n",
    "    return result, rho\n",
    "\n",
    "def sample_denoising(data, k=10, num_sample=500, omega=0.2, metric='euclidean'):    \n",
    "    \"\"\"Fuzzy UMAP sampling with greedy farthest point selection.\"\"\"\n",
    "    n = data.shape[0]\n",
    "    F_D = np.zeros(n)\n",
    "    \n",
    "    X = squareform(pdist(data, metric))\n",
    "    knn_indices = np.argsort(X)[:, :k]\n",
    "    knn_dists = X[np.arange(X.shape[0])[:, None], knn_indices].copy()\n",
    "\n",
    "    sigmas, rhos = smooth_knn_dist(knn_dists, k, local_connectivity=0)\n",
    "    rows, cols, vals = compute_membership_strengths(knn_indices, knn_dists, sigmas, rhos)\n",
    "    result = coo_matrix((vals, (rows, cols)), shape=(n, n))\n",
    "    result.eliminate_zeros()\n",
    "    transpose = result.transpose()\n",
    "    prod_matrix = result.multiply(transpose)\n",
    "    result = (result + transpose - prod_matrix)\n",
    "    result.eliminate_zeros()\n",
    "    X = result.toarray()\n",
    "    F = np.sum(X, 1)\n",
    "    \n",
    "    # Greedy sampling\n",
    "    Fs = np.zeros(num_sample)\n",
    "    Fs[0] = np.max(F)\n",
    "    i = np.argmax(F)\n",
    "    inds_all = np.arange(n)\n",
    "    inds_left = inds_all > -1\n",
    "    inds_left[i] = False\n",
    "    inds = np.zeros(num_sample, dtype=int)\n",
    "    inds[0] = i\n",
    "    \n",
    "    for j in np.arange(1, num_sample):\n",
    "        F -= omega * X[i, :]\n",
    "        Fmax = np.argmax(F[inds_left])\n",
    "        Fs[j] = F[Fmax]\n",
    "        i = inds_all[inds_left][Fmax]\n",
    "        inds_left[i] = False   \n",
    "        inds[j] = i\n",
    "        \n",
    "    d = np.zeros((num_sample, num_sample))\n",
    "    for j, i in enumerate(inds):\n",
    "        d[j, :] = X[i, inds]\n",
    "        \n",
    "    return inds, d, Fs\n",
    "\n",
    "# RUN ME\n",
    "# edited from Gardner et al. (2022)\n",
    "def plot_barcode(persistence, file_name = '', name = ''):\n",
    "    diagrams_roll = {}\n",
    "    filenames=glob.glob('Results/Roll/' + file_name + '_H2_roll_*')\n",
    "    for i, fname in enumerate(filenames): \n",
    "        f = np.load(fname, allow_pickle = True)\n",
    "        diagrams_roll[i] = list(f['diagrams'])\n",
    "        f.close() \n",
    "\n",
    "\n",
    "    cs = np.repeat([[0,0.55,0.2]],3).reshape(3,3).T\n",
    "    alpha=1\n",
    "    inf_delta=0.1\n",
    "    legend=True\n",
    "    colormap=cs\n",
    "    maxdim = len(persistence) - 1\n",
    "    dims =np.arange(maxdim+1)\n",
    "    num_rolls = len(diagrams_roll)\n",
    "\n",
    "\n",
    "    if num_rolls>0:\n",
    "        diagrams_all = np.copy(diagrams_roll[0])\n",
    "        for i in np.arange(1,num_rolls):\n",
    "            for d in dims:\n",
    "                diagrams_all[d] = np.concatenate((diagrams_all[d], diagrams_roll[i][d]),0)\n",
    "        infs = np.isinf(diagrams_all[0])\n",
    "        diagrams_all[0][infs] = 0\n",
    "        diagrams_all[0][infs] = np.max(diagrams_all[0])\n",
    "        infs = np.isinf(diagrams_all[0])\n",
    "        diagrams_all[0][infs] = 0\n",
    "        diagrams_all[0][infs] = np.max(diagrams_all[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    min_birth, max_death = 0,0            \n",
    "    for dim in dims:\n",
    "        persistence_dim = persistence[dim][~np.isinf(persistence[dim][:,1]),:]\n",
    "        min_birth = min(min_birth, np.min(persistence_dim))\n",
    "        max_death = max(max_death, np.max(persistence_dim))\n",
    "    delta = (max_death - min_birth) * inf_delta\n",
    "    infinity = max_death + delta\n",
    "    axis_start = min_birth - delta            \n",
    "    plotind = (dims[-1]+1)*100 + 10 +1\n",
    "    fig = plt.figure()\n",
    "    gs = grd.GridSpec(len(dims),1)\n",
    "\n",
    "\n",
    "    indsall =  0\n",
    "    labels = [\"$H_0$\", \"$H_1$\", \"$H_2$\", \"$H_3$\"] #, \"$H_2$\"\n",
    "    for dit, dim in enumerate(dims):\n",
    "        axes = plt.subplot(gs[dim])\n",
    "        #axes.axis('off')\n",
    "        axes.set_ylabel(f\"H{dim}\", fontsize=8)\n",
    "        d = np.copy(persistence[dim])\n",
    "        d[np.isinf(d[:,1]),1] = infinity\n",
    "        dlife = (d[:,1] - d[:,0])\n",
    "        dinds = np.argsort(dlife)[-30:]\n",
    "        #dl1,dl2 = dlife[dinds[-2:]]\n",
    "        if dim>0:\n",
    "            dinds = dinds[np.flip(np.argsort(d[dinds,0]))]\n",
    "        axes.barh(\n",
    "            0.5+np.arange(len(dinds)),\n",
    "            dlife[dinds],\n",
    "            height=0.8,\n",
    "            left=d[dinds,0],\n",
    "            alpha=alpha,\n",
    "            color=colormap[dim],\n",
    "            linewidth=0,\n",
    "        )\n",
    "        indsall = len(dinds)\n",
    "        if num_rolls>0:\n",
    "            bins = 50\n",
    "            cs = np.flip([[0.4,0.4,0.4], [0.6,0.6,0.6], [0.8, 0.8,0.8]])\n",
    "            cs = np.repeat([[1,0.55,0.1]],3).reshape(3,3).T\n",
    "            cc = 0\n",
    "            lives1_all = diagrams_all[dim][:,1] - diagrams_all[dim][:,0]\n",
    "            x1 = np.linspace(diagrams_all[dim][:,0].min()-1e-5, diagrams_all[dim][:,0].max()+1e-5, bins-2)\n",
    "            \n",
    "            dx1 = (x1[1] - x1[0])\n",
    "            x1 = np.concatenate(([x1[0]-dx1], x1, [x1[-1]+dx1]))\n",
    "            dx = x1[:-1] + dx1/2\n",
    "            ytemp = np.zeros((bins-1))\n",
    "            binned_birth = np.digitize(diagrams_all[dim][:,0], x1)-1\n",
    "            x1  = d[dinds,0]\n",
    "            ytemp =x1 + np.max(lives1_all)\n",
    "            axes.fill_betweenx(0.5+np.arange(len(dinds)), x1, ytemp, color = cs[(dim)], zorder = -2, alpha = 0.3)\n",
    "\n",
    "\n",
    "        axes.plot([0,0], [0, indsall], c = 'k', linestyle = '-', lw = 1)\n",
    "        axes.plot([0,indsall],[0,0], c = 'k', linestyle = '-', lw = 1)\n",
    "        axes.tick_params(labelsize=5)\n",
    "        axes.set_yticks([])\n",
    "        axes.set_xlim([0, infinity])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af235d82",
   "metadata": {},
   "source": [
    "## 2. Check datas tensor sizes for downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81affd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C155: 136,000\n",
      "    Large tensor - recommend downsampling for reasonable computation time\n",
      "  C159: 136,000\n",
      "    Large tensor - recommend downsampling for reasonable computation time\n",
      "  C161: 136,000\n",
      "    Large tensor - recommend downsampling for reasonable computation time\n",
      "  C155: 136,000 to 351\n",
      "  C159: 136,000 to 351\n",
      "  C161: 136,000 to 351\n"
     ]
    }
   ],
   "source": [
    "# Check data sizes and recommend downsampling\n",
    "for name, X in zip(recordings, datas):\n",
    "    n = X.shape[0]\n",
    "    print(f\"  {name}: {n:,}\")\n",
    "    \n",
    "    if n > 50000:\n",
    "        print(f\"    Large tensor - recommend downsampling for reasonable computation time\")\n",
    "    elif n > 20000:\n",
    "        print(f\"    Medium tensor - consider downsampling\")\n",
    "    else:\n",
    "        print(f\"    Should be okay for direct processing\")\n",
    "\n",
    "#  downsampling parameters\n",
    "target_size = 350  # target after downsampling\n",
    "downsample_factor = None  # computed automatically\n",
    "\n",
    "downsampled_datas = []\n",
    "\n",
    "for name, X in zip(recordings, datas):\n",
    "    original_size = X.shape[0]\n",
    "    \n",
    "    if original_size > target_size:\n",
    "        # calculate optimal downsampling factor\n",
    "        factor = max(1, original_size // target_size)\n",
    "        \n",
    "        if isinstance(X, torch.Tensor):\n",
    "            down = X[::factor, :]\n",
    "        else:\n",
    "            down = X[::factor].copy()\n",
    "            \n",
    "        downsampled_datas.append(down)\n",
    "        reduction_pct = (1 - down.shape[0]/original_size) * 100\n",
    "        print(f\"  {name}: {original_size:,} to {down.shape[0]:,}\")\n",
    "    else:\n",
    "        # No downsampling needed\n",
    "        downsampled_datas.append(X)\n",
    "        print(f\"  {name}: {original_size:,} (no downsampling needed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3df364",
   "metadata": {},
   "source": [
    "## 3. Run Fuzzy UMAP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6e06628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing session: C155\n",
      "  Input shape: torch.Size([351, 15])\n",
      "Running PCA\n",
      "  PCA: 15 → 15 dimensions (100.0% variance)\n",
      "  Adjusted parameters: k=50 neighbors, 175 sample points\n",
      "  Running fuzzy UMAP sampling\n",
      "  Sampled 175 points\n",
      "  Constructing KNN graph (k=50)\n",
      "  Negative log transformation\n",
      "  Distance matrix shape: (351, 351)\n",
      "  Computing persistence diagrams (max dim=2)\n",
      "  Sampled 175 points\n",
      "  Constructing KNN graph (k=50)\n",
      "  Negative log transformation\n",
      "  Distance matrix shape: (351, 351)\n",
      "  Computing persistence diagrams (max dim=2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation minimum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Computing persistence diagrams (max dim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmaxdim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m persistence \u001b[38;5;241m=\u001b[39m ripser(d, maxdim\u001b[38;5;241m=\u001b[39mmaxdim, coeff\u001b[38;5;241m=\u001b[39mcoeff, do_cocycles\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, distance_matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 86\u001b[0m plot_barcode(persistence[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdgms\u001b[39m\u001b[38;5;124m'\u001b[39m], file_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Store results\u001b[39;00m\n\u001b[1;32m     89\u001b[0m all_results[name] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpersistence\u001b[39m\u001b[38;5;124m'\u001b[39m: persistence,\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdgms\u001b[39m\u001b[38;5;124m'\u001b[39m: persistence[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdgms\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_sampled\u001b[39m\u001b[38;5;124m'\u001b[39m: n_sample \u001b[38;5;28;01mif\u001b[39;00m n_points \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m n_points\n\u001b[1;32m     96\u001b[0m }\n",
      "Cell \u001b[0;32mIn[4], line 183\u001b[0m, in \u001b[0;36mplot_barcode\u001b[0;34m(persistence, file_name, name)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m dims:\n\u001b[1;32m    182\u001b[0m     persistence_dim \u001b[38;5;241m=\u001b[39m persistence[dim][\u001b[38;5;241m~\u001b[39mnp\u001b[38;5;241m.\u001b[39misinf(persistence[dim][:,\u001b[38;5;241m1\u001b[39m]),:]\n\u001b[0;32m--> 183\u001b[0m     min_birth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(min_birth, np\u001b[38;5;241m.\u001b[39mmin(persistence_dim))\n\u001b[1;32m    184\u001b[0m     max_death \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(max_death, np\u001b[38;5;241m.\u001b[39mmax(persistence_dim))\n\u001b[1;32m    185\u001b[0m delta \u001b[38;5;241m=\u001b[39m (max_death \u001b[38;5;241m-\u001b[39m min_birth) \u001b[38;5;241m*\u001b[39m inf_delta\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:3343\u001b[0m, in \u001b[0;36mmin\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_min_dispatcher)\n\u001b[1;32m   3226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   3227\u001b[0m         where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   3228\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3229\u001b[0m \u001b[38;5;124;03m    Return the minimum of an array or minimum along an axis.\u001b[39;00m\n\u001b[1;32m   3230\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3341\u001b[0m \u001b[38;5;124;03m    6\u001b[39;00m\n\u001b[1;32m   3342\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapreduction(a, np\u001b[38;5;241m.\u001b[39mminimum, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out,\n\u001b[1;32m   3344\u001b[0m                           keepdims\u001b[38;5;241m=\u001b[39mkeepdims, initial\u001b[38;5;241m=\u001b[39minitial, where\u001b[38;5;241m=\u001b[39mwhere)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation minimum which has no identity"
     ]
    }
   ],
   "source": [
    "# main processing pipeline\n",
    "\n",
    "# params - can be adjusted\n",
    "ph_classes = [0, 1, 2]  # homology groups to compute\n",
    "metric = 'cosine'      \n",
    "maxdim = 2              # maximum homology group dimension\n",
    "coeff = 2               # coefficient field (2 = orientation doesn't matter)\n",
    "\n",
    "# Process each session\n",
    "all_results = {}\n",
    "\n",
    "for name, X in zip(recordings, downsampled_datas):\n",
    "    print(f\"\\nProcessing session: {name}\")\n",
    "    print(f\"  Input shape: {X.shape}\")\n",
    "    \n",
    "    # Convert to numpy \n",
    "    if isinstance(X, torch.Tensor):\n",
    "        X_np = X.numpy()\n",
    "    else:\n",
    "        X_np = X\n",
    "    \n",
    "    # Standardize data\n",
    "    X_scaled = StandardScaler().fit_transform(X_np)\n",
    "\n",
    "    # PCA\n",
    "    print(\"Running PCA\")\n",
    "    pca_full = PCA(svd_solver='auto', random_state=42).fit(X_scaled)\n",
    "    sum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "    k = max(32, np.searchsorted(sum_var, 0.9) + 1)  # set k for 90% variance explained\n",
    "    k = min(k, min(X_scaled.shape))  # don't exceed data dimensions\n",
    "\n",
    "    pca = PCA(n_components=k, svd_solver='auto', random_state=42)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    print(f\"  PCA: {X.shape[1]} → {k} dimensions ({sum_var[k-1]:.1%} variance)\")\n",
    "\n",
    "    # Adjust parameters based on actual data size\n",
    "    n_points = X_pca.shape[0]\n",
    "    nbs = min(50, n_points - 10)  # Use fewer neighbors, leave room for sampling\n",
    "    n_sample = min(200, n_points // 2)  # Sample at most half the data\n",
    "    \n",
    "    print(f\"  Adjusted parameters: k={nbs} neighbors, {n_sample} sample points\")\n",
    "    \n",
    "    # Skip fuzzy UMAP sampling if data is too small - use direct distance matrix instead\n",
    "    if n_points < 100:\n",
    "        print(\"  Small dataset - using direct distance matrix approach\")\n",
    "        dist_mat = squareform(pdist(X_pca, metric))\n",
    "    else:\n",
    "        # Fuzzy UMAP sampling and denoising\n",
    "        print(\"  Running fuzzy UMAP sampling\")\n",
    "        try:\n",
    "            indstemp, dd, fs = sample_denoising(X_pca, k=nbs, num_sample=n_sample, omega=1, metric=metric)\n",
    "            print(f\"  Sampled {len(indstemp)} points\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Fuzzy sampling failed: {e}, falling back to direct approach\")\n",
    "            dist_mat = squareform(pdist(X_pca, metric))\n",
    "        else:\n",
    "            # Build distance matrix from PCA data\n",
    "            dist_mat = squareform(pdist(X_pca, metric))\n",
    "\n",
    "    # Construct fuzzy simplicial set\n",
    "    print(f\"  Constructing KNN graph (k={nbs})\")\n",
    "    knn_indices = np.argsort(dist_mat)[:, :nbs]\n",
    "    knn_dists = dist_mat[np.arange(dist_mat.shape[0])[:, None], knn_indices].copy()\n",
    "    sigmas, rhos = smooth_knn_dist(knn_dists, nbs, local_connectivity=0)\n",
    "\n",
    "    # Compute membership strengths\n",
    "    rows, cols, vals = compute_membership_strengths(knn_indices, knn_dists, sigmas, rhos)\n",
    "    result = coo_matrix((vals, (rows, cols)), shape=(dist_mat.shape[0], dist_mat.shape[0]))\n",
    "    result.eliminate_zeros()\n",
    "    transpose = result.transpose()\n",
    "    prod_matrix = result.multiply(transpose)\n",
    "    result = (result + transpose - prod_matrix)\n",
    "    result.eliminate_zeros()\n",
    "    d = result.toarray()\n",
    "\n",
    "    # Transform to distance matrix\n",
    "    print(\"  Negative log transformation\")\n",
    "    d = np.clip(d, 1e-12, None)  \n",
    "    d = -np.log(d)\n",
    "    np.fill_diagonal(d, 0)  \n",
    "    print(f\"  Distance matrix shape: {d.shape}\")\n",
    "\n",
    "    # Compute persistence diagrams\n",
    "    print(f\"  Computing persistence diagrams (max dim={maxdim})\")\n",
    "    persistence = ripser(d, maxdim=maxdim, coeff=coeff, do_cocycles=True, distance_matrix=True)\n",
    "    plot_barcode(persistence['dgms'], file_name=\"\")\n",
    "    \n",
    "    # Store results\n",
    "    all_results[name] = {\n",
    "        'persistence': persistence,\n",
    "        'dgms': persistence['dgms'],\n",
    "        'n_original': X.shape[0],\n",
    "        'n_processed': X_pca.shape[0],\n",
    "        'n_pca_dims': k,\n",
    "        'n_sampled': n_sample if n_points >= 100 else n_points\n",
    "    }\n",
    "    \n",
    "    # Print summary \n",
    "    dgms = persistence['dgms']\n",
    "    print(f\"  Summary:\")\n",
    "    for i, dgm in enumerate(dgms):\n",
    "        if dgm is not None and len(dgm) > 0:\n",
    "            # filter out infinite bars for H0\n",
    "            finite_bars = dgm[np.isfinite(dgm).all(axis=1)]\n",
    "            if len(finite_bars) > 0:\n",
    "                max_persistence = np.max(finite_bars[:, 1] - finite_bars[:, 0])\n",
    "                print(f\"    H{i}: {len(dgm)} features (max persistence: {max_persistence:.3f})\")\n",
    "            else:\n",
    "                print(f\"    H{i}: {len(dgm)} features (all infinite)\")\n",
    "        else:\n",
    "            print(f\"    H{i}: 0 features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94f8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results \n",
    "if all_results:\n",
    "    try:\n",
    "        for session_name, results in all_results.items():\n",
    "            # set filename\n",
    "            base_name = dataset_info.get('filename', 'spike_data').replace('.pkl', '')\n",
    "            filename = f\"FuzzyUMAP_barcode_{base_name}_{session_name}.pkl\"\n",
    "            \n",
    "            # save to persistence examples directory\n",
    "            output_path = data_manager.get_output_path(filename, 'persistence_diagram_examples')\n",
    "            \n",
    "            # put data to save in a dictionary\n",
    "            save_data = {\n",
    "                'dgms': results['dgms'],\n",
    "                'persistence': results['persistence'],\n",
    "                'metadata': {\n",
    "                    'session': session_name,\n",
    "                    'method': 'Fuzzy UMAP + Ripser',\n",
    "                    'n_original_timepoints': results['n_original'],\n",
    "                    'n_pca_dimensions': results['n_pca_dims'],\n",
    "                    'n_sampled_points': results['n_sampled'],\n",
    "                    'metric': metric,\n",
    "                    'max_homology_dim': maxdim,\n",
    "                    'coefficient_field': coeff,\n",
    "                    'dataset': dataset_info.get('filename', 'unknown')\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            with open(output_path, 'wb') as f:\n",
    "                pkl.dump(save_data, f)\n",
    "            \n",
    "            print(f\"{session_name}: {output_path.name}\")\n",
    "        \n",
    "        print(f\"files saved to: {data_manager.data_paths['persistence_examples']}\")\n",
    "        \n",
    "        # summary of saved files\n",
    "        saved_files = data_manager.find_files(\"FuzzyUMAP_barcode_*.pkl\", \"persistence_examples\")\n",
    "        print(f\"total saved barcode files: {len(saved_files)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error saving results: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"No results to save. Run the processing cells above first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
