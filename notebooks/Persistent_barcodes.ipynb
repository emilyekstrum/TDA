{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc4320be",
   "metadata": {},
   "source": [
    "# Persistence Barcodes from Neural Spike Data\n",
    "\n",
    "Generates persistence barcodes using fuzzy UMAP algorithm from the toroidal grid cell paper.\n",
    "\n",
    "- **Input:** Cleaned spike data automatically loaded from the TDA data manager\n",
    "- **Output:** Persistence barcodes for homology groups H0-H2\n",
    "- **Method:** Fuzzy simplicial sets ‚Üí distance matrix ‚Üí Ripser TDA\n",
    "\n",
    "Author: @emilyekstrum & Gardner et al. (2022)\n",
    "<br> 11/25/25\n",
    "\n",
    "**Reference:** Gardner, R.J., Hermansen, E., Pachitariu, M. et al. Toroidal topology of population activity in grid cells. *Nature* 602, 123‚Äì128 (2022). https://doi.org/10.1038/s41586-021-04268-7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3febdfb",
   "metadata": {},
   "source": [
    "## Fuzzy UMAP Methodology Overview\n",
    "\n",
    "The pipeline implements the fuzzy UMAP approach from Gardner et al.:\n",
    "\n",
    "1. **Fuzzy KNN Graph Construction**: Build weighted graph using greedy farthest point sampling\n",
    "   - Select most connected points, down-weight nearby points\n",
    "   - Ensures coverage of less-dense data regions\n",
    "\n",
    "2. **Distance & KNN Graph**: Compute pairwise distances and k-nearest neighbors\n",
    "\n",
    "3. **UMAP-style Scale Estimation**: Find optimal sigma (scaling) and rho (nearest neighbor distance)\n",
    "   - Ensures soft membership ‚â• log‚ÇÇ(k) for topology preservation\n",
    "\n",
    "4. **Fuzzy Simplicial Set**: Convert KNN ‚Üí sparse adjacency matrix ‚Üí directed fuzzy graph\n",
    "\n",
    "5. **Greedy Sampling**: Identify highly connected points using fuzzy similarities\n",
    "\n",
    "6. **TDA**: Use fuzzy similarity matrix as input to Ripser for persistence computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4db90ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import glob\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from ripser import Rips, ripser\n",
    "from scipy import stats, signal, optimize\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.linalg import eigh\n",
    "from scipy.sparse.linalg import lsmr\n",
    "from sklearn import preprocessing\n",
    "\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "plt.style.use(['default', 'seaborn-v0_8-paper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa03b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spike data already available.\n",
      "Extracting persistence diagrams from all_dgms.zip...\n",
      "Extracted to: /Users/emilyekstrum/repos/TDA/data/all_dgms\n",
      "TDA utilities loaded\n",
      "TDA Data Manager Summary\n",
      "Workspace root: /Users/emilyekstrum/repos/TDA\n",
      "Data directory: /Users/emilyekstrum/repos/TDA/data\n",
      "  - clean_spike_data_zip: clean_spike_data.zip\n",
      "  - clean_spike_data_dir: clean_spike_data\n",
      "      Contains 8 .pkl files\n",
      "  - cebra_examples: CEBRA_embedding_examples\n",
      "      Contains 4 .pkl files\n",
      "  - persistence_examples: persistence_diagram_examples\n",
      "      Contains 22 .pkl files\n",
      "  - all_dgms_zip: all_dgms.zip\n",
      "  X all_dgms_dir: all_dgms\n",
      "\n",
      "Available spike datasets (8):\n",
      "  ‚Ä¢ LGN_chromatic_gratings.pkl\n",
      "  ‚Ä¢ LGN_color_exchange.pkl\n",
      "  ‚Ä¢ LGN_drifting_gratings.pkl\n",
      "  ‚Ä¢ LGN_luminance_flash.pkl\n",
      "  ‚Ä¢ V1_chromatic_gratings.pkl\n",
      "  and 3 more\n"
     ]
    }
   ],
   "source": [
    "# Import TDA utilities\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the repo root to path if needed\n",
    "repo_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "try:\n",
    "    from tda_utils import TDADataManager, tda_manager\n",
    "    print(\"TDA utilities loaded\")\n",
    "except ImportError as e:\n",
    "    print(f\"Could not import TDA utilities: {e}\")\n",
    "    print(\"Make sure you're running from the TDA repository.\")\n",
    "    raise\n",
    "\n",
    "# Initialize or use the global data manager\n",
    "if tda_manager is not None:\n",
    "    data_manager = tda_manager\n",
    "else:\n",
    "    data_manager = TDADataManager()\n",
    "\n",
    "# Print summary of available data\n",
    "data_manager.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b54e16d",
   "metadata": {},
   "source": [
    "## 1. Load in spike data and inspect files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efb126d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple datasets available:\n",
      "  1. LGN_chromatic_gratings.pkl\n",
      "  2. LGN_color_exchange.pkl\n",
      "  3. LGN_drifting_gratings.pkl\n",
      "  4. LGN_luminance_flash.pkl\n",
      "  5. V1_chromatic_gratings.pkl\n",
      "  6. V1_color_exchange.pkl\n",
      "  7. V1_drifting_gratings.pkl\n",
      "  8. V1_luminance_flash.pkl\n",
      "\n",
      "Using the first one. To specify, use dataset_name parameter.\n",
      "Loading dataset: LGN_chromatic_gratings.pkl\n",
      "Loaded 3 recording sessions\n",
      "  Sessions: ['C155', 'C159', 'C161']\n",
      "  Data shape: torch.Size([136000, 15])\n",
      "Dataset: LGN_chromatic_gratings.pkl\n",
      "Sessions: 3\n",
      "Session IDs: ['C155', 'C159', 'C161']\n",
      "Data shapes:\n",
      "  C155: torch.Size([136000, 15]) (torch.Tensor)\n",
      "  C159: torch.Size([136000, 177]) (torch.Tensor)\n",
      "  C161: torch.Size([136000, 95]) (torch.Tensor)\n"
     ]
    }
   ],
   "source": [
    "# Load spike data \n",
    "try:\n",
    "    datas, recordings, dataset_info = data_manager.load_spike_data()\n",
    "    print(f\"Dataset: {dataset_info['filename']}\")\n",
    "    print(f\"Sessions: {dataset_info['n_sessions']}\")\n",
    "    print(f\"Session IDs: {recordings}\")\n",
    "    \n",
    "    print(\"Data shapes:\")\n",
    "    for name, X in zip(recordings, datas):\n",
    "        print(f\"  {name}: {X.shape} ({'torch.Tensor' if isinstance(X, torch.Tensor) else 'numpy.ndarray'})\")\n",
    "    \n",
    "    # To load a specific dataset, use:\n",
    "    # datas, recordings, dataset_info = data_manager.load_spike_data('LGN_color_exchange.pkl')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not load data: {e}\")\n",
    "    print(\"\\nTo fix issue:\")\n",
    "    print(\"1. Make sure you're in the TDA repository\")\n",
    "    print(\"2. Check that data/clean_spike_data.zip exists\")\n",
    "    print(\"3. Or manually add .pkl files to data/clean_spike_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ec16724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuzzy UMAP functions from Gardner et al. (2022)\n",
    "# RUN ME\n",
    "\n",
    "import numba\n",
    "from datetime import datetime \n",
    "import functools\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.ndimage import gaussian_filter, gaussian_filter1d, binary_dilation, binary_closing\n",
    "\n",
    "_LOG_2PI = np.log(2 * np.pi)\n",
    "\n",
    "# Core UMAP functions (from UMAP library, optimized with numba)\n",
    "@numba.njit(parallel=True, fastmath=True) \n",
    "def compute_membership_strengths(knn_indices, knn_dists, sigmas, rhos):\n",
    "    \"\"\"Compute fuzzy membership strengths for KNN graph.\"\"\"\n",
    "    n_samples = knn_indices.shape[0]\n",
    "    n_neighbors = knn_indices.shape[1]\n",
    "    rows = np.zeros((n_samples * n_neighbors), dtype=np.int64)\n",
    "    cols = np.zeros((n_samples * n_neighbors), dtype=np.int64)\n",
    "    vals = np.zeros((n_samples * n_neighbors), dtype=np.float64)\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_neighbors):\n",
    "            if knn_indices[i, j] == -1:\n",
    "                continue  # We didn't get the full knn for i\n",
    "            if knn_indices[i, j] == i:\n",
    "                val = 0.0\n",
    "            elif knn_dists[i, j] - rhos[i] <= 0.0:\n",
    "                val = 1.0\n",
    "            else:\n",
    "                val = np.exp(-((knn_dists[i, j] - rhos[i]) / (sigmas[i])))\n",
    "\n",
    "            rows[i * n_neighbors + j] = i\n",
    "            cols[i * n_neighbors + j] = knn_indices[i, j]\n",
    "            vals[i * n_neighbors + j] = val\n",
    "\n",
    "    return rows, cols, vals\n",
    "\n",
    "@numba.njit(fastmath=True)\n",
    "def smooth_knn_dist(distances, k, n_iter=64, local_connectivity=0.0, bandwidth=1.0):\n",
    "    \"\"\"Smooth KNN distances using UMAP methodology.\"\"\"\n",
    "    target = np.log2(k) * bandwidth\n",
    "    \n",
    "    rho = np.zeros(distances.shape[0])\n",
    "    result = np.zeros(distances.shape[0])\n",
    "\n",
    "    mean_distances = np.mean(distances)\n",
    "\n",
    "    for i in range(distances.shape[0]):\n",
    "        lo = 0.0\n",
    "        hi = np.inf\n",
    "        mid = 1.0\n",
    "\n",
    "        ith_distances = distances[i]\n",
    "        non_zero_dists = ith_distances[ith_distances > 0.0]\n",
    "        if non_zero_dists.shape[0] >= local_connectivity:\n",
    "            index = int(np.floor(local_connectivity))\n",
    "            interpolation = local_connectivity - index\n",
    "            if index > 0:\n",
    "                rho[i] = non_zero_dists[index - 1]\n",
    "                if interpolation > 1e-5:\n",
    "                    rho[i] += interpolation * (\n",
    "                        non_zero_dists[index] - non_zero_dists[index - 1]\n",
    "                    )\n",
    "            else:\n",
    "                rho[i] = interpolation * non_zero_dists[0]\n",
    "        elif non_zero_dists.shape[0] > 0:\n",
    "            rho[i] = np.max(non_zero_dists)\n",
    "\n",
    "        for n in range(n_iter):\n",
    "            psum = 0.0\n",
    "            for j in range(1, distances.shape[1]):\n",
    "                d = distances[i, j] - rho[i]\n",
    "                if d > 0:\n",
    "                    psum += np.exp(-(d / mid))\n",
    "                else:\n",
    "                    psum += 1.0\n",
    "\n",
    "            if np.fabs(psum - target) < 1e-5:\n",
    "                break\n",
    "\n",
    "            if psum > target:\n",
    "                hi = mid\n",
    "                mid = (lo + hi) / 2.0\n",
    "            else:\n",
    "                lo = mid\n",
    "                if hi == np.inf:\n",
    "                    mid *= 2\n",
    "                else:\n",
    "                    mid = (lo + hi) / 2.0\n",
    "        result[i] = mid\n",
    "\n",
    "        if rho[i] > 0.0:\n",
    "            mean_ith_distances = np.mean(ith_distances)\n",
    "            if result[i] < 1e-3 * mean_ith_distances:\n",
    "                result[i] = 1e-3 * mean_ith_distances\n",
    "        else:\n",
    "            if result[i] < 1e-3 * mean_distances:\n",
    "                result[i] = 1e-3 * mean_distances\n",
    "\n",
    "    return result, rho\n",
    "\n",
    "def sample_denoising(data, k=10, num_sample=500, omega=0.2, metric='euclidean'):    \n",
    "    \"\"\"Fuzzy UMAP sampling with greedy farthest point selection.\"\"\"\n",
    "    n = data.shape[0]\n",
    "    F_D = np.zeros(n)\n",
    "    \n",
    "    X = squareform(pdist(data, metric))\n",
    "    knn_indices = np.argsort(X)[:, :k]\n",
    "    knn_dists = X[np.arange(X.shape[0])[:, None], knn_indices].copy()\n",
    "\n",
    "    sigmas, rhos = smooth_knn_dist(knn_dists, k, local_connectivity=0)\n",
    "    rows, cols, vals = compute_membership_strengths(knn_indices, knn_dists, sigmas, rhos)\n",
    "    result = coo_matrix((vals, (rows, cols)), shape=(n, n))\n",
    "    result.eliminate_zeros()\n",
    "    transpose = result.transpose()\n",
    "    prod_matrix = result.multiply(transpose)\n",
    "    result = (result + transpose - prod_matrix)\n",
    "    result.eliminate_zeros()\n",
    "    X = result.toarray()\n",
    "    F = np.sum(X, 1)\n",
    "    \n",
    "    # Greedy sampling\n",
    "    Fs = np.zeros(num_sample)\n",
    "    Fs[0] = np.max(F)\n",
    "    i = np.argmax(F)\n",
    "    inds_all = np.arange(n)\n",
    "    inds_left = inds_all > -1\n",
    "    inds_left[i] = False\n",
    "    inds = np.zeros(num_sample, dtype=int)\n",
    "    inds[0] = i\n",
    "    \n",
    "    for j in np.arange(1, num_sample):\n",
    "        F -= omega * X[i, :]\n",
    "        Fmax = np.argmax(F[inds_left])\n",
    "        Fs[j] = F[Fmax]\n",
    "        i = inds_all[inds_left][Fmax]\n",
    "        inds_left[i] = False   \n",
    "        inds[j] = i\n",
    "        \n",
    "    d = np.zeros((num_sample, num_sample))\n",
    "    for j, i in enumerate(inds):\n",
    "        d[j, :] = X[i, inds]\n",
    "        \n",
    "    return inds, d, Fs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af235d82",
   "metadata": {},
   "source": [
    "## 2. Check datas tensor sizes for downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81affd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  C155: 136,000\n",
      "    Large tensor - recommend downsampling for reasonable computation time\n",
      "  C159: 136,000\n",
      "    Large tensor - recommend downsampling for reasonable computation time\n",
      "  C161: 136,000\n",
      "    Large tensor - recommend downsampling for reasonable computation time\n",
      "  C155: 136,000 to 10,462\n",
      "  C159: 136,000 to 10,462\n",
      "  C161: 136,000 to 10,462\n"
     ]
    }
   ],
   "source": [
    "# Check data sizes and recommend downsampling\n",
    "for name, X in zip(recordings, datas):\n",
    "    n = X.shape[0]\n",
    "    print(f\"  {name}: {n:,}\")\n",
    "    \n",
    "    if n > 50000:\n",
    "        print(f\"    Large tensor - recommend downsampling for reasonable computation time\")\n",
    "    elif n > 20000:\n",
    "        print(f\"    Medium tensor - consider downsampling\")\n",
    "    else:\n",
    "        print(f\"    Should be okay for direct processing\")\n",
    "\n",
    "#  downsampling parameters\n",
    "target_size = 10000  # target after downsampling\n",
    "downsample_factor = None  # computed automatically\n",
    "\n",
    "downsampled_datas = []\n",
    "\n",
    "for name, X in zip(recordings, datas):\n",
    "    original_size = X.shape[0]\n",
    "    \n",
    "    if original_size > target_size:\n",
    "        # calculate optimal downsampling factor\n",
    "        factor = max(1, original_size // target_size)\n",
    "        \n",
    "        if isinstance(X, torch.Tensor):\n",
    "            down = X[::factor, :]\n",
    "        else:\n",
    "            down = X[::factor].copy()\n",
    "            \n",
    "        downsampled_datas.append(down)\n",
    "        reduction_pct = (1 - down.shape[0]/original_size) * 100\n",
    "        print(f\"  {name}: {original_size:,} to {down.shape[0]:,}\")\n",
    "    else:\n",
    "        # No downsampling needed\n",
    "        downsampled_datas.append(X)\n",
    "        print(f\"  {name}: {original_size:,} (no downsampling needed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3df364",
   "metadata": {},
   "source": [
    "## 3. Run Fuzzy UMAP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6e06628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large dataset: using 500\n",
      "Using k=100 for fuzzy graph construction\n",
      "\n",
      "Processing session: C155\n",
      "  Input shape: torch.Size([10462, 15])\n",
      "Running PCA\n",
      "  PCA: 15 - 15 dimensions (100.0% variance)\n",
      "  Constructing KNN graph (k=100)\n",
      "  Negative log transformation\n",
      "  Persistence diagrams (max dim=2)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Main processing pipeline with optimized parameters\n",
    "\n",
    "# Parameters optimized - can be adjusted\n",
    "ph_classes = [0, 1, 2]  # homology groups to compute\n",
    "metric = 'cosine'      \n",
    "maxdim = 2              # maximum homology group dimension\n",
    "coeff = 2               # coefficient field (2 = orientation doesn't matter)\n",
    "\n",
    "# set parameters based on data size\n",
    "sample_data_size = downsampled_datas[0].shape[0]\n",
    "\n",
    "if sample_data_size > 5000:\n",
    "    n = min(500, sample_data_size // 20)  # aggressive sampling\n",
    "    print(f\"Large dataset: using {n}\")\n",
    "elif sample_data_size > 1000:\n",
    "    n_points = min(300, sample_data_size // 10)  # moderate sampling\n",
    "    print(f\"Medium dataset mode: using {n}\")\n",
    "else:\n",
    "    n_points = min(200, sample_data_size // 5)   # minimal sampling\n",
    "    print(f\"Small dataset mode: using {n}\")\n",
    "\n",
    "nbs = max(10, n // 5)  # Number of neighbors \n",
    "print(f\"Using k={nbs} for fuzzy graph construction\")\n",
    "\n",
    "# Process each session\n",
    "all_results = {}\n",
    "\n",
    "for name, X in zip(recordings, downsampled_datas):\n",
    "    print(f\"\\nProcessing session: {name}\")\n",
    "    print(f\"  Input shape: {X.shape}\")\n",
    "    \n",
    "    # Convert to numpy \n",
    "    if isinstance(X, torch.Tensor):\n",
    "        X_np = X.numpy()\n",
    "    else:\n",
    "        X_np = X\n",
    "    \n",
    "    # Standardize data\n",
    "    X_scaled = StandardScaler().fit_transform(X_np)\n",
    "\n",
    "    # PCA\n",
    "    print(\"Running PCA\")\n",
    "    pca_full = PCA(svd_solver='auto', random_state=42).fit(X_scaled)\n",
    "    sum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "    k = max(32, np.searchsorted(sum_var, 0.9) + 1)  # set k for 90% variance explained\n",
    "    k = min(k, min(X_scaled.shape))  # don't exceed data dimensions\n",
    "\n",
    "    pca = PCA(n_components=k, svd_solver='auto', random_state=42)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    print(f\"  PCA: {X.shape[1]} - {k} dimensions ({sum_var[k-1]:.1%} variance)\")\n",
    "\n",
    "    # Fuzzy UMAP sampling and denoising\n",
    "    indstemp, dd, fs = sample_denoising(X_pca, k=nbs, omega=1, metric=metric)\n",
    "\n",
    "    # Build distance matrix from PCA data\n",
    "    dist_mat = squareform(pdist(X_pca, metric))\n",
    "\n",
    "    # Construct fuzzy simplicial set\n",
    "    print(f\"  Constructing KNN graph (k={nbs})\")\n",
    "    knn_indices = np.argsort(dist_mat)[:, :nbs]\n",
    "    knn_dists = dist_mat[np.arange(dist_mat.shape[0])[:, None], knn_indices].copy()\n",
    "    sigmas, rhos = smooth_knn_dist(knn_dists, nbs, local_connectivity=0)\n",
    "\n",
    "    # Compute membership strengths\n",
    "    rows, cols, vals = compute_membership_strengths(knn_indices, knn_dists, sigmas, rhos)\n",
    "    result = coo_matrix((vals, (rows, cols)), shape=(dist_mat.shape[0], dist_mat.shape[0]))\n",
    "    result.eliminate_zeros()\n",
    "    transpose = result.transpose()\n",
    "    prod_matrix = result.multiply(transpose)\n",
    "    result = (result + transpose - prod_matrix)\n",
    "    result.eliminate_zeros()\n",
    "    d = result.toarray()\n",
    "\n",
    "    # Transform to distance matrix\n",
    "    print(\"  Negative log transformation\")\n",
    "    d = np.clip(d, 1e-12, None)  \n",
    "    d = -np.log(d)\n",
    "    np.fill_diagonal(d, 0)  \n",
    "    print(f\"  Distance matrix shape: {d.shape}\")\n",
    "\n",
    "    #  persistence diagrams\n",
    "    print(f\"  Persistence diagrams (max dim={maxdim})\")\n",
    "    persistence = ripser(d, maxdim=maxdim, coeff=coeff, do_cocycles=True, distance_matrix=True) # uses cocycles - better at capturing twistedness in data\n",
    "    \n",
    "    # Store results\n",
    "    all_results[name] = {\n",
    "        'persistence': persistence,\n",
    "        'dgms': persistence['dgms'],\n",
    "        'n_original': X.shape[0],\n",
    "        'n_processed': X_pca.shape[0],\n",
    "        'n_pca_dims': k,\n",
    "        'n_sampled': n_points\n",
    "    }\n",
    "    \n",
    "    # summary \n",
    "    dgms = persistence['dgms']\n",
    "    print(f\"  Summary:\")\n",
    "    for i, dgm in enumerate(dgms):\n",
    "        if dgm is not None and len(dgm) > 0:\n",
    "            # filter out infinite bars for H0\n",
    "            finite_bars = dgm[np.isfinite(dgm).all(axis=1)]\n",
    "            if len(finite_bars) > 0:\n",
    "                max_persistence = np.max(finite_bars[:, 1] - finite_bars[:, 0])\n",
    "                print(f\"    H{i}: {len(dgm)} features (max persistence: {max_persistence:.3f})\")\n",
    "            else:\n",
    "                print(f\"    H{i}: {len(dgm)} features (all infinite)\")\n",
    "        else:\n",
    "            print(f\"    H{i}: 0 features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b696fd",
   "metadata": {},
   "source": [
    "## 4. Plot persistence barcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdaa068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ME\n",
    "# edited from Gardner et al. (2022)\n",
    "def plot_barcode(persistence, file_name = '', name = ''):\n",
    "    diagrams_roll = {}\n",
    "    filenames=glob.glob('Results/Roll/' + file_name + '_H2_roll_*')\n",
    "    for i, fname in enumerate(filenames): \n",
    "        f = np.load(fname, allow_pickle = True)\n",
    "        diagrams_roll[i] = list(f['diagrams'])\n",
    "        f.close() \n",
    "\n",
    "\n",
    "    cs = np.repeat([[0,0.55,0.2]],3).reshape(3,3).T\n",
    "    alpha=1\n",
    "    inf_delta=0.1\n",
    "    legend=True\n",
    "    colormap=cs\n",
    "    maxdim = len(persistence) - 1\n",
    "    dims =np.arange(maxdim+1)\n",
    "    num_rolls = len(diagrams_roll)\n",
    "\n",
    "\n",
    "    if num_rolls>0:\n",
    "        diagrams_all = np.copy(diagrams_roll[0])\n",
    "        for i in np.arange(1,num_rolls):\n",
    "            for d in dims:\n",
    "                diagrams_all[d] = np.concatenate((diagrams_all[d], diagrams_roll[i][d]),0)\n",
    "        infs = np.isinf(diagrams_all[0])\n",
    "        diagrams_all[0][infs] = 0\n",
    "        diagrams_all[0][infs] = np.max(diagrams_all[0])\n",
    "        infs = np.isinf(diagrams_all[0])\n",
    "        diagrams_all[0][infs] = 0\n",
    "        diagrams_all[0][infs] = np.max(diagrams_all[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    min_birth, max_death = 0,0            \n",
    "    for dim in dims:\n",
    "        persistence_dim = persistence[dim][~np.isinf(persistence[dim][:,1]),:]\n",
    "        min_birth = min(min_birth, np.min(persistence_dim))\n",
    "        max_death = max(max_death, np.max(persistence_dim))\n",
    "    delta = (max_death - min_birth) * inf_delta\n",
    "    infinity = max_death + delta\n",
    "    axis_start = min_birth - delta            \n",
    "    plotind = (dims[-1]+1)*100 + 10 +1\n",
    "    fig = plt.figure()\n",
    "    gs = grd.GridSpec(len(dims),1)\n",
    "\n",
    "\n",
    "    indsall =  0\n",
    "    labels = [\"$H_0$\", \"$H_1$\", \"$H_2$\", \"$H_3$\"] #, \"$H_2$\"\n",
    "    for dit, dim in enumerate(dims):\n",
    "        axes = plt.subplot(gs[dim])\n",
    "        #axes.axis('off')\n",
    "        axes.set_ylabel(f\"H{dim}\", fontsize=8)\n",
    "        d = np.copy(persistence[dim])\n",
    "        d[np.isinf(d[:,1]),1] = infinity\n",
    "        dlife = (d[:,1] - d[:,0])\n",
    "        dinds = np.argsort(dlife)[-30:]\n",
    "        dl1,dl2 = dlife[dinds[-2:]]\n",
    "        if dim>0:\n",
    "            dinds = dinds[np.flip(np.argsort(d[dinds,0]))]\n",
    "        axes.barh(\n",
    "            0.5+np.arange(len(dinds)),\n",
    "            dlife[dinds],\n",
    "            height=0.8,\n",
    "            left=d[dinds,0],\n",
    "            alpha=alpha,\n",
    "            color=colormap[dim],\n",
    "            linewidth=0,\n",
    "        )\n",
    "        indsall = len(dinds)\n",
    "        if num_rolls>0:\n",
    "            bins = 50\n",
    "            cs = np.flip([[0.4,0.4,0.4], [0.6,0.6,0.6], [0.8, 0.8,0.8]])\n",
    "            cs = np.repeat([[1,0.55,0.1]],3).reshape(3,3).T\n",
    "            cc = 0\n",
    "            lives1_all = diagrams_all[dim][:,1] - diagrams_all[dim][:,0]\n",
    "            x1 = np.linspace(diagrams_all[dim][:,0].min()-1e-5, diagrams_all[dim][:,0].max()+1e-5, bins-2)\n",
    "            \n",
    "            dx1 = (x1[1] - x1[0])\n",
    "            x1 = np.concatenate(([x1[0]-dx1], x1, [x1[-1]+dx1]))\n",
    "            dx = x1[:-1] + dx1/2\n",
    "            ytemp = np.zeros((bins-1))\n",
    "            binned_birth = np.digitize(diagrams_all[dim][:,0], x1)-1\n",
    "            x1  = d[dinds,0]\n",
    "            ytemp =x1 + np.max(lives1_all)\n",
    "            axes.fill_betweenx(0.5+np.arange(len(dinds)), x1, ytemp, color = cs[(dim)], zorder = -2, alpha = 0.3)\n",
    "\n",
    "\n",
    "        axes.plot([0,0], [0, indsall], c = 'k', linestyle = '-', lw = 1)\n",
    "        axes.plot([0,indsall],[0,0], c = 'k', linestyle = '-', lw = 1)\n",
    "        axes.tick_params(labelsize=5)\n",
    "        axes.set_yticks([])\n",
    "        axes.set_xlim([0, infinity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ff12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_barcode(dgms, file_name='example_session', name='Example Session')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94f8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results \n",
    "if all_results:\n",
    "    print(\"üíæ Saving persistence barcode results...\")\n",
    "    \n",
    "    try:\n",
    "        for session_name, results in all_results.items():\n",
    "            # Create descriptive filename\n",
    "            base_name = dataset_info.get('filename', 'spike_data').replace('.pkl', '')\n",
    "            filename = f\"FuzzyUMAP_barcode_{base_name}_{session_name}.pkl\"\n",
    "            \n",
    "            # Save to persistence examples directory\n",
    "            output_path = data_manager.get_output_path(filename, 'persistence_diagram_examples')\n",
    "            \n",
    "            # Prepare data for saving\n",
    "            save_data = {\n",
    "                'dgms': results['dgms'],\n",
    "                'persistence': results['persistence'],\n",
    "                'metadata': {\n",
    "                    'session': session_name,\n",
    "                    'method': 'Fuzzy UMAP + Ripser',\n",
    "                    'n_original_timepoints': results['n_original'],\n",
    "                    'n_pca_dimensions': results['n_pca_dims'],\n",
    "                    'n_sampled_points': results['n_sampled'],\n",
    "                    'metric': metric,\n",
    "                    'max_homology_dim': maxdim,\n",
    "                    'coefficient_field': coeff,\n",
    "                    'dataset': dataset_info.get('filename', 'unknown')\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            with open(output_path, 'wb') as f:\n",
    "                pkl.dump(save_data, f)\n",
    "            \n",
    "            print(f\"  ‚úÖ {session_name}: {output_path.name}\")\n",
    "        \n",
    "        print(f\"\\nüéâ All results saved successfully!\")\n",
    "        print(f\"üìÅ Location: {data_manager.data_paths['persistence_examples']}\")\n",
    "        \n",
    "        # Show summary of saved files\n",
    "        saved_files = data_manager.find_files(\"FuzzyUMAP_barcode_*.pkl\", \"persistence_examples\")\n",
    "        print(f\"üìã Total saved barcode files: {len(saved_files)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving results: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results to save. Run the processing cells above first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
